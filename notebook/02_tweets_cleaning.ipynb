{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54bca08",
   "metadata": {},
   "source": [
    "## 2. Preprocessing of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aa394",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b177b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer,AutoModelForSeq2SeqLM, pipeline\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20faf880",
   "metadata": {},
   "source": [
    "### Regex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_filename = r'tweets_[a-z]*_[a-z]{2}'\n",
    "pattern_keyword = r'_([a-z]+)_[a-z]{2}.csv'\n",
    "pattern_lang = r'_([a-z]{2}).csv'\n",
    "# pattern to remove url, \\n, \\xxx, #hastag, @user, and \\\n",
    "pattern_sub = r'https://t.co/\\w{9,10}|\\\\n|\\\\x\\w{2}|#\\w+|@\\w+|\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1991d",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06c227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of tweets per keyword and language\n",
    "def stats_keyword_lang(filepath):\n",
    "    keyword = []\n",
    "    lang = []\n",
    "    nrow = []\n",
    "    nrow_non_null = []\n",
    "    for filepath in tqdm(glob.glob(filepath)):\n",
    "        temp_df = pd.read_csv(filepath, lineterminator= '\\n', encoding= 'latin-1')\n",
    "        keyword += [re.search(pattern_keyword, filepath).group(1)]\n",
    "        lang += [re.search(pattern_lang, filepath).group(1)]\n",
    "        nrow += [temp_df.shape[0]]\n",
    "        nrow_non_null += [temp_df[~temp_df['text'].isna()].shape[0]]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'keyword':keyword,\n",
    "        'lang':lang,\n",
    "        'nrow':nrow,\n",
    "        'nrow_non_null':nrow_non_null\n",
    "    })\n",
    "    df['pct_non_null'] = df['nrow_non_null']/df['nrow']*100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e8e55",
   "metadata": {},
   "source": [
    "### Concatenate csv per keyword and language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d856729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select sebset of tweets for four keywords in fr, es, and pt\n",
    "keywords = ['ukraine', 'russia', 'zelenskyy', 'putin']\n",
    "langs = ['fr', 'es', 'pt']\n",
    "for keyword in keywords:\n",
    "    for lang in langs:\n",
    "        df = pd.DataFrame()\n",
    "        for filepath in tqdm(glob.glob(f'../data/tweets/tweets_{keyword}/tweets_{keyword}_{lang}/*')):\n",
    "            df_temp = pd.read_csv(filepath, lineterminator='\\n', encoding= 'latin-1')\n",
    "            df_temp = df_temp.dropna(subset=['location'])  # remove tweets with NaN in location\n",
    "            df_temp = df_temp[:500] # select 500 tweets per day, keyword, language\n",
    "            df = pd.concat([df, df_temp]).sort_values('date')\n",
    "        df = df.dropna(subset = ['tweet_id', 'text']).drop_duplicates('text')  # remove tweets with NaN in tweet_id or text and drop duplicated text\n",
    "        df.to_csv(f'../data/tweets_final/tweets_{keyword}_{lang}.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per keyword and language\n",
    "filepath = '../data/tweets_final/*'\n",
    "df = stats_keyword_lang(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c9dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values('nrow_non_null', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82861205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('keyword').sum().sort_values('nrow_non_null', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4f4bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('lang').sum().sort_values('nrow_non_null', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3661816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['lang']!='en']['nrow_non_null'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab5fa5",
   "metadata": {},
   "source": [
    "### Clean tweets\n",
    "Remove b and '', url, \\n, \\xxx, #hastag, @user, and \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b1ef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filepath in tqdm(glob.glob('../data/tweets_final/*')):\n",
    "    df = pd.read_csv(filepath,lineterminator='\\n', encoding= 'latin-1')\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(pattern_sub, '', repr(str(x).strip(\"b'\")).strip(\"'\")))\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106dbc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per keyword and language\n",
    "filepath = '../data/tweets_final/*'\n",
    "df = stats_keyword_lang(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78d8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values('nrow_non_null', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8e0c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby('keyword').sum().sort_values('nrow_non_null', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['lang']!='en']['nrow_non_null'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3e906",
   "metadata": {},
   "source": [
    "### Select subset of tweets and delete potential fake accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete potential fake accounts\n",
    "keywords = ['ukraine', 'russia', 'zelenskyy', 'putin']\n",
    "for keyword in keywords:\n",
    "    for filepath in tqdm(glob.glob(f'../data/tweets_final/tweets_{keyword}*')):\n",
    "        df = pd.read_csv(filepath,lineterminator='\\n', encoding= 'latin-1')    \n",
    "        # Select tweets before 2022-04-16\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "        df = df[df['date']<='2022-04-16']\n",
    "        # Delete potential fake accounts\n",
    "        df['friends_ount'] = pd.to_numeric(df['friends_ount'], errors= 'coerce').fillna(0)\n",
    "        df['followers_ount'] = pd.to_numeric(df['followers_ount'], errors= 'coerce').fillna(0) \n",
    "        df = df[~((df['friends_ount'] <= 20)&( df['followers_ount'] <= 5)& df['acct_desc'].isnull())]\n",
    "        filepath_new = re.sub('tweets_final', 'tweets_translated', filepath)\n",
    "        df.to_csv(filepath_new, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e75a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per keyword and language\n",
    "filepath = '../data/tweets_translated/*'\n",
    "df = stats_keyword_lang(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('lang').sum().sort_values('nrow_non_null', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['lang']!='en']['nrow_non_null'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ada15",
   "metadata": {},
   "source": [
    "### Translation of tweets to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained models\n",
    "model_es_en = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
    "model_pt_en = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "model_fr_en = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "\n",
    "# Pretrained tokenizers\n",
    "tokenizer_es_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
    "tokenizer_pt_en = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "tokenizer_fr_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "\n",
    "# Translation pipelines\n",
    "pipeline_es_en = pipeline(\"translation_es_to_en\", model=model_es_en, tokenizer=tokenizer_es_en)\n",
    "pipeline_pt_en = pipeline(\"text2text-generation\", model=model_pt_en, tokenizer=tokenizer_pt_en)\n",
    "pipeline_fr_en = pipeline(\"translation_fr_to_en\", model=model_fr_en, tokenizer=tokenizer_fr_en)\n",
    "\n",
    "def translate_es_en(text):\n",
    "    translated_text = pipeline_es_en(text, max_length=400)[0]['translation_text']\n",
    "    return translated_text\n",
    "def translate_pt_en(text):\n",
    "    translated_text = pipeline_pt_en(text, max_length=400)[0]['generated_text']\n",
    "    return translated_text\n",
    "def translate_fr_en(text):\n",
    "    translated_text = pipeline_fr_en(text, max_length=400)[0]['translation_text']\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb43c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "langs = ['es', 'pt', 'fr']\n",
    "for lang in langs:\n",
    "    filepaths = glob.glob(f'../data/tweets_translated/*_{lang}.csv')\n",
    "    for filepath in tqdm(filepaths):\n",
    "        print(filepath)\n",
    "        df = pd.read_csv(filepath,lineterminator='\\n', encoding= 'latin-1')\n",
    "        if lang == 'es':\n",
    "            df['text'] = df['text'].apply(translate_es_en)\n",
    "            df['location'] = df['location'].apply(lambda x: translate_es_en(x).strip('. ') if not pd.isna(x) else x)\n",
    "        elif lang == 'pt':\n",
    "            df['text'] = df['text'].apply(translate_pt_en)\n",
    "            df['location'] = df['location'].apply(lambda x: translate_pt_en(x).strip('. ') if not pd.isna(x) else x)\n",
    "        elif lang == 'fr':\n",
    "            df['text'] = df['text'].apply(translate_fr_en)\n",
    "            df['location'] = df['location'].apply(lambda x: translate_fr_en(x).strip('. ') if not pd.isna(x) else x)\n",
    "        else:\n",
    "            df['text'] = df['text'].apply(translate_de_en)\n",
    "            df['location'] = df['location'].apply(lambda x: translate_de_en(x).strip('. ') if not pd.isna(x) else x)\n",
    "        df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01aa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per keyword and language\n",
    "filepath = '../data/tweets_translated/*'\n",
    "df = stats_keyword_lang(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1e685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('keyword').sum().sort_values('nrow_non_null', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7d6e3",
   "metadata": {},
   "source": [
    "### Concatenate translated tweets per keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6582b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate translated tweets per keywords\n",
    "keywords = ['ukraine', 'russia', 'zelenskyy', 'putin']\n",
    "for keyword in keywords:\n",
    "    df = pd.DataFrame()\n",
    "    for filepath in glob.glob(f'../data/tweets_translated/tweets_{keyword}_*.csv'):\n",
    "        df_temp = pd.read_csv(filepath, lineterminator='\\n', encoding= 'latin-1')\n",
    "        df = pd.concat([df, df_temp]).sort_values('date')\n",
    "    df.to_csv(f'../data/tweets_translated/tweets_{keyword}_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd2557",
   "metadata": {},
   "source": [
    "### Combine English tweets with non-English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_keep = ['tweet_id', 'acct_desc', 'date', 'location', 'friends_ount','followers_ount', 'text', 'compound']\n",
    "for keyword in keywords:\n",
    "    df = pd.read_csv(f'../data/tweets_en/tweets_{keyword}_en.csv', usecols=col_to_keep, lineterminator='\\n', encoding='latin-1')\n",
    "    df2 = pd.read_csv(f'../data/tweets_translated/tweets_{keyword}_translated.csv', lineterminator='\\n', encoding='latin-1')\n",
    "    pd.concat([df, df2]).to_csv(f'../data/tweets_en/tweets_{keyword}_en.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
