{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af6e646",
   "metadata": {},
   "source": [
    "## Building Topic Model with the Gensim Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d8affe8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate =pd.to_datetime('2022-02-24').date()\n",
    "keywords = ['ukraine', 'russia', 'eu', 'zelenskyy', 'biden', 'putin', 'johnson', 'nato', 'scholz', 'macron']\n",
    "\n",
    "for keyword in tqdm(keywords):\n",
    "    df = pd.read_csv(f'../data/tweets_en/tweets_{keyword}_en.csv',dtype={'date':'str'}, parse_dates = ['date'], lineterminator='\\n', encoding='latin-1')\n",
    "    df['date'] = pd.DatetimeIndex(df['date']).date\n",
    "    df = df[df['date']>startdate].sample(10000)\n",
    "\n",
    "    docs = df['text'].tolist()\n",
    "\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "    # Remove stopwords\n",
    "    docs = [[token for token in doc if token not in STOPWORDS] for doc in docs]\n",
    "\n",
    "    # Lemmatize the documents.\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "    # Compute bigrams.\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=5)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "\n",
    "    # Remove rare and common tokens.\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    ## Latent Dirichlet Allocation\n",
    "\n",
    "    # Train LDA model.\n",
    "   \n",
    "    # Set training parameters.\n",
    "    num_topics = 5\n",
    "    chunksize = 100\n",
    "    passes = 10\n",
    "    iterations = 50\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every,\n",
    "        random_state=321\n",
    "    )\n",
    "\n",
    "    vis = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "    pyLDAvis.save_html(vis, f'{keyword}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e93ab1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf3f350b2ed4791bd0271ac9a3057be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contents = []\n",
    "\n",
    "for title, abstract, idx, doc in tqdm(zip(papers['title'].to_list(),\n",
    "                                     papers['abstract'].to_list(),\n",
    "                                     papers['id'].to_list(),\n",
    "                                     docs)):\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    topics = model.get_document_topics(bow)\n",
    "    for res in topics:\n",
    "        topic, pct = res\n",
    "        contents.append({'id': idx,\n",
    "                         'title': title, \n",
    "                         'abstract': abstract,\n",
    "                         'topic': topic, \n",
    "                         'percent': pct})\n",
    "\n",
    "topics = pd.DataFrame(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351fe8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3044d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[topics['topic'] == 3].sort_values('percent', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9247db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2203.15236\n",
    "\n",
    "print(topics[topics['id'] == idx].iloc[0, 1])\n",
    "topics[topics['id'] == idx].iloc[0, 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
