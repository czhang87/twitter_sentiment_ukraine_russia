{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af6e646",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling with Gensim LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3f480",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8affe8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe1c90",
   "metadata": {},
   "source": [
    "### Create a list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords = STOPWORDS.union(set(stopwords))\n",
    "stopwords = stopwords.union(set(['tell','come','lot','took','youre','thats','got','said','im','maybe','mr','he','oh','today','let','amp', 'need', 'know', 'going', 'think', 'want', 'year', 'day', 'time', 'dont', 'thing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7e15e",
   "metadata": {},
   "source": [
    "### LDA modeling and visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate =pd.to_datetime('2022-02-24').date()\n",
    "keywords = ['ukraine', 'russia', 'eu', 'zelenskyy', 'biden', 'putin', 'johnson', 'nato', 'scholz', 'macron']\n",
    "\n",
    "for keyword in tqdm(keywords):\n",
    "    df = pd.read_csv(f'../data/tweets_en/tweets_{keyword}_en.csv',dtype={'date':'str'}, parse_dates = ['date'], lineterminator='\\n', encoding='latin-1')\n",
    "    df['date'] = pd.DatetimeIndex(df['date']).date\n",
    "    df = df[df['date']>startdate].sample(10000)\n",
    "\n",
    "    docs = df['text'].tolist()\n",
    "\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "    # Remove stopwords\n",
    "    docs = [[token for token in doc if token not in stopwords] for doc in docs]\n",
    "\n",
    "    # Lemmatize the documents.\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "    # Compute bigrams.\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=5)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "\n",
    "    # Remove rare and common tokens.\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    ## Latent Dirichlet Allocation\n",
    "\n",
    "    # Train LDA model.\n",
    "   \n",
    "    # Set training parameters.\n",
    "    num_topics = 5\n",
    "    chunksize = 100\n",
    "    passes = 10\n",
    "    iterations = 50\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every,\n",
    "        random_state=321\n",
    "    )\n",
    "\n",
    "    vis = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "    pyLDAvis.save_html(vis, f'../shinyapp/www/{keyword}.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
