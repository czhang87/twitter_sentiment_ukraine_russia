{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4baf809a",
   "metadata": {},
   "source": [
    "## 4. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca738c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93840f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.reset_option('max_rows')\n",
    "# pd.reset_option('max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1bdc7",
   "metadata": {},
   "source": [
    "### Convert location to iso2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c17b8471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# geographical data\n",
    "geo_countries = gpd.read_file('../data/countries.geojson')\n",
    "world_cities = pd.read_csv('../data/worldcities/worldcities.csv', usecols= ['city_ascii', 'country', 'iso2', 'population'])\n",
    "city_iso2 = world_cities.sort_values('population', ascending = False).drop_duplicates('city_ascii').rename(columns={'city_ascii':'city'})[['city', 'iso2']]\n",
    "state_iso2 = pd.read_csv('../data/US_states.csv', usecols=['STATE', 'STATE2']).assign(iso2='USA').rename(columns = {'STATE':'state', 'STATE2':'state2'})\n",
    "country_iso2 = world_cities.sort_values('population', ascending = False).drop_duplicates('iso2')[['country', 'iso2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0286c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace the inconsistent locations with country names\n",
    "US = ['US', 'USA', 'Southern California', 'United States of America', 'West Coast, USA (SEA/GEG/LAX)',\n",
    "       'Democrat in the USA','Washington DC', 'New York City', 'NYC', 'USA (SEA/GEG/LAX)', 'Washington DC',\n",
    "       'Lost in the Minnesota North Woods', 'Washington D.C.', 'U.S.A.', 'U.S.A', 'us', 'New England', 'Philly','D.C.'\n",
    "       ,'usa', 'Northern Virginia', 'American', 'South Florida','Pacific Northwest', 'Northern California', \n",
    "       'San Diego & New Orleans', 'San Francisco Bay Area','New York and the World', 'America','Washington State', \n",
    "       'Midwest','East Coast', 'US of A', 'Harrisburg Pa']\n",
    "UK = ['UK','England', 'Scotland', 'london', 'Wales', 'uk', 'West Saxons', 'LONDON', 'South London', 'Northern Ireland',\n",
    "       'Somerset England UK Eu', 'Derbyshire', 'England and International', 'London UK','Leicestershire','Lancashire'\n",
    "       ,'West Sussex','North Yorkshire','Yorkshire and The Humber, Engl','Yorkshire and The Humber','Newcastle upon Tyne'\n",
    "       ,'Staffordshire','Oxfordshire','Cardiff','West Yorkshire','Cymru','Great Britain','North Wales','Wiltshire'\n",
    "       ,'Stockport','North West England','Cambridgeshire','glasgow','Romford','SCOTLAND','South Wales','Dorset'\n",
    "       ,'some were in UK','england','Hertfordshire','Shropshire England','North East England','Hampshire UK'\n",
    "       ,'Norfolk England','Dorset, England.','Bristol UK','Republic of Wales','U.K.','Lancashire','East Sussex'\n",
    "       ,'Warwickshire', 'Edinburgh ~ Heart of Scotland!']\n",
    "Germany = [\"Deutschland\", 'Somewhere in Germany']\n",
    "Ukraine = ['Ucraina', 'chernivtsi', 'Some Future Place in Ukraine', 'Mariouple']\n",
    "Poland = ['Polska']\n",
    "Canada = ['CANADA']\n",
    "Netherlands = ['The Netherlands']\n",
    "Czechia = ['Czech Republic']\n",
    "Italy = ['Italia']    \n",
    "Brazil = ['Brasil', 'Brazil Brazil']   \n",
    "Finland = ['Suomi', \"East-Finnish People's Republic\"]    \n",
    "Japan = ['Okinotorishima Ogasawara Tokyo']   \n",
    "Australia = ['Queensland','Australia - International']   \n",
    "Sweden = ['Sverige']  \n",
    "Kazakhstan = ['Astana']  \n",
    "Serbia = ['Belgrade City'] \n",
    "India = ['mumbai']   \n",
    "Belgium = ['Belgique', 'Bruxelles']  \n",
    "Ethiopia = ['Tigray']   \n",
    "Venezuela = ['Venezuela revolucionaria']\n",
    "Spain = ['Tarragona']\n",
    "Greece = ['Athens Greece']\n",
    "Denmark = ['Danmark']\n",
    "\n",
    "def convert_loc(text):\n",
    "    if text in US:\n",
    "        return 'United States'\n",
    "    if text in UK:\n",
    "        return 'United Kingdom'\n",
    "    if text in Germany:\n",
    "        return 'Germany'\n",
    "    if text in Ukraine:\n",
    "        return 'Ukraine'\n",
    "    if text in Poland:\n",
    "        return 'Poland'\n",
    "    if text in Canada:\n",
    "        return 'Canada'\n",
    "    if text in Netherlands:\n",
    "        return 'Netherlands'\n",
    "    if text in Czechia:\n",
    "        return 'Czechia'\n",
    "    if text in Italy:\n",
    "        return 'Italy'\n",
    "    if text in Brazil:\n",
    "        return 'Brazil'\n",
    "    if text in Finland:\n",
    "        return 'Finland'\n",
    "    if text in Japan:\n",
    "        return 'Japan'\n",
    "    if text in Australia:\n",
    "        return 'Australia'\n",
    "    if text in Sweden:\n",
    "        return 'Sweden'\n",
    "    if text in Kazakhstan:\n",
    "        return 'Kazakhstan'\n",
    "    if text in Serbia:\n",
    "        return 'Serbia'\n",
    "    if text in India:\n",
    "        return 'India'\n",
    "    if text in Belgium:\n",
    "        return 'Belgium'\n",
    "    if text in Ethiopia:\n",
    "        return 'Ethiopia'\n",
    "    if text in Venezuela:\n",
    "        return 'Venezuela'\n",
    "    if text in Spain:\n",
    "        return 'Spain'\n",
    "    if text in Greece:\n",
    "        return 'Greece'\n",
    "    if text in Denmark:\n",
    "        return 'Denmark'\n",
    "    else:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa8d05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd4d40e5b58470dbf31a8a1e0b3da88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_keep = ['tweet_id', 'acct_desc', 'date', 'location', 'friends_ount','followers_ount', 'text', 'compound', 'iso2_final']\n",
    "for filepath in tqdm(glob.glob('../data/tweets_en/*')):\n",
    "    df = pd.read_csv(filepath, lineterminator='\\n', encoding='latin-1')\n",
    "    \n",
    "    # split the location by comma and assign them to loc1 and loc2 \n",
    "    loc_split = df['location'].apply(lambda x : str(x).split(','))\n",
    "    loc1 = []\n",
    "    loc2 = []\n",
    "    for loc in loc_split:\n",
    "        if len(loc) == 1:\n",
    "            loc1 += [loc[0]]\n",
    "            loc2 += ['nan']\n",
    "        if len(loc) == 2:\n",
    "            loc1 += [loc[0]]\n",
    "            loc2 += [loc[1]]\n",
    "        if len(loc) >2:\n",
    "            loc1 += [loc[0]]\n",
    "            loc2 += [loc[1]]\n",
    "    df = df.assign(loc1 = loc1, loc2 = loc2)\n",
    "    \n",
    "    # strip the blank space in loc1 and loc2; convert the inconsistent loc1 and loc2 to country names\n",
    "    df['loc1'] = df['loc1'].apply(lambda x: x.strip(' '))\n",
    "    df['loc1'] = df['loc1'].apply(convert_loc)\n",
    "    df['loc2'] = df['loc2'].apply(lambda x: x.strip(' '))\n",
    "    df['loc2'] = df['loc2'].apply(convert_loc)\n",
    "    \n",
    "    # merge location columns with iso2 dataframes to match city, state, or country\n",
    "    df = (df.\n",
    "     merge(city_iso2, how = 'left', left_on = 'loc1', right_on = 'city').\n",
    "     merge(city_iso2, how = 'left', left_on = 'loc2', right_on = 'city').\n",
    "     merge(state_iso2, how = 'left', left_on = 'loc1', right_on = 'state').\n",
    "     merge(state_iso2, how = 'left', left_on = 'loc2', right_on = 'state').\n",
    "     merge(state_iso2, how = 'left', left_on = 'loc1', right_on = 'state2').\n",
    "     merge(state_iso2, how = 'left', left_on = 'loc2', right_on = 'state2').\n",
    "     merge(country_iso2, how = 'left', left_on = 'loc1', right_on = 'country').\n",
    "     merge(country_iso2, how = 'left', left_on = 'loc2', right_on = 'country')\n",
    "    )\n",
    "    \n",
    "    # ignore the null values and get the iso2\n",
    "    df['iso2_final'] = (df['iso2_x'].iloc[:,0].\n",
    "     combine_first(df['iso2_x'].iloc[:,1]).\n",
    "     combine_first(df['iso2_x'].iloc[:,2]).\n",
    "     combine_first(df['iso2_x'].iloc[:,3]).\n",
    "     combine_first(df['iso2_y'].iloc[:,0]).\n",
    "     combine_first(df['iso2_y'].iloc[:,1]).\n",
    "     combine_first(df['iso2_y'].iloc[:,2]).\n",
    "     combine_first(df['iso2_y'].iloc[:,3])\n",
    "    )\n",
    "    df = df[col_to_keep]# drop iso2_ columns\n",
    "    df.to_csv(filepath, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743af6d",
   "metadata": {},
   "source": [
    "### Calculate sentiment scores and number of tweets per country before and after the war (2/24/22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b5a1ff6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c52d773d1d489cbe1f4320b8ef21dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "startdate =pd.to_datetime('2022-02-24').date()\n",
    "keywords = ['ukraine', 'russia', 'eu', 'zelenskyy', 'biden', 'putin', 'johnson', 'nato', 'scholz', 'macron']\n",
    "before_war = pd.DataFrame({'iso2_final':[]}).astype({'iso2_final':'str'})\n",
    "after_war = pd.DataFrame({'iso2_final':[]}).astype({'iso2_final':'str'})\n",
    "\n",
    "for keyword in tqdm(keywords):\n",
    "    df = pd.read_csv(f'../data/tweets_en/tweets_{keyword}_en.csv',dtype={'date':'str'}, parse_dates = ['date'], lineterminator='\\n', encoding='latin-1')\n",
    "    df['date'] = pd.DatetimeIndex(df['date']).date\n",
    "    \n",
    "    # before the war\n",
    "    df_temp = (df[(df['date']< startdate)&(abs(df['compound'])>0.1)].\n",
    "                  groupby('iso2_final').\n",
    "                  mean()[['compound']].\n",
    "                  reset_index().\n",
    "                  rename(columns={'compound':f'compound_{keyword}_before_war'}))\n",
    "    df_temp[f'compound_{keyword}_before_war_count'] = (df[(df['date']< startdate)&(abs(df['compound'])>0.1)].\n",
    "            groupby('iso2_final').\n",
    "            count()['compound']).to_list()\n",
    "    before_war = before_war.merge(df_temp, how='outer', left_on='iso2_final', right_on='iso2_final')\n",
    "    \n",
    "    # after the war\n",
    "    df_temp = (df[(df['date']> startdate)&(abs(df['compound'])>0.1)].\n",
    "                  groupby('iso2_final').\n",
    "                  mean()[['compound']].\n",
    "                  reset_index().\n",
    "                  rename(columns={'compound':f'compound_{keyword}_after_war'}))\n",
    "    df_temp[f'compound_{keyword}_after_war_count'] = (df[(df['date']> startdate)&(abs(df['compound'])>0.1)].\n",
    "            groupby('iso2_final').\n",
    "            count()['compound']).to_list()\n",
    "    after_war = after_war.merge(df_temp, how='outer', left_on='iso2_final', right_on='iso2_final')\n",
    "    \n",
    "df_temp = before_war.merge(after_war, how='outer', left_on='iso2_final', right_on='iso2_final')\n",
    "df_temp.dropna(axis='columns', how='all').to_csv('../shinyapp/data/sentiment_per_country.csv')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
